# ğŸ¦™ Ollama Kurulum ve KullanÄ±m KÄ±lavuzu

## Ollama Nedir?

Ollama, yerel olarak Ã§alÄ±ÅŸan aÃ§Ä±k kaynaklÄ± LLM (Large Language Model) platformudur. Gemini gibi bulut tabanlÄ± API'larÄ±n aksine:

âœ… **Tamamen Ã¼cretsiz** - API key veya Ã¼cret yok
âœ… **Yerel** - Internet baÄŸlantÄ±sÄ± gerekmez
âœ… **HÄ±zlÄ±** - DÃ¼ÅŸÃ¼k latency
âœ… **Gizlilik** - Verileriniz dÄ±ÅŸarÄ± Ã§Ä±kmaz
âœ… **Rate limit yok** - SÄ±nÄ±rsÄ±z kullanÄ±m

## Kurulum

### macOS
```bash
# Homebrew ile kurulum
brew install ollama

# Manuel kurulum
curl -fsSL https://ollama.com/install.sh | sh
```

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows
[Ollama.com](https://ollama.com/download) sitesinden indirin.

## Ollama BaÅŸlatma

```bash
# Ollama servisini baÅŸlat
ollama serve
```

VarsayÄ±lan olarak `http://localhost:11434` adresinde Ã§alÄ±ÅŸÄ±r.

## Model Ä°ndirme

Uygulama kullanmadan Ã¶nce en az bir model indirmeniz gerekir:

### Ã–nerilen Modeller

**HÄ±zlÄ± ve KÃ¼Ã§Ã¼k:**
```bash
ollama pull qwen2.5-coder:1.5b    # 1.5B parametreli, Ã§ok hÄ±zlÄ±
ollama pull phi4-mini:3.8b-fp16   # 3.8B, iyi performans
ollama pull gemma3:4b             # 4B, dengeli
```

**Orta Seviye:**
```bash
ollama pull qwen2.5:7b           # 7B, iyi kalite
ollama pull llama3.2:8b          # 8B, Meta'nÄ±n modeli
```

**YÃ¼ksek Kalite:**
```bash
ollama pull gemma3:12b           # 12B, yÃ¼ksek doÄŸruluk
ollama pull deepseek-r1:14b      # 14B, Ã§ok gÃ¼Ã§lÃ¼
ollama pull gpt-oss:20b          # 20B, GPT alternatifi
```

### Model BoyutlarÄ± ve Gereksinimler

| Model | Boyut | RAM | HÄ±z | Kalite |
|-------|-------|-----|-----|--------|
| qwen2.5-coder:1.5b | ~1GB | 4GB | âš¡âš¡âš¡ | â­â­ |
| phi4-mini:3.8b | ~2.5GB | 6GB | âš¡âš¡ | â­â­â­ |
| qwen2.5:7b | ~5GB | 8GB | âš¡âš¡ | â­â­â­â­ |
| gemma3:12b | ~8GB | 12GB | âš¡ | â­â­â­â­â­ |
| deepseek-r1:14b | ~10GB | 16GB | âš¡ | â­â­â­â­â­ |

## Uygulama Entegrasyonu

### 1. Otomatik Model SeÃ§imi

Uygulama baÅŸladÄ±ÄŸÄ±nda Ollama'daki ilk modeli otomatik seÃ§er:

```bash
./start.sh
```

Log'larda gÃ¶receksiniz:
```
Auto-selected first available model: gemma3:12b
```

### 2. Web ArayÃ¼zÃ¼nden Model DeÄŸiÅŸtirme

1. TarayÄ±cÄ±da `http://localhost:5001` adresine gidin
2. **System Status** bÃ¶lÃ¼mÃ¼nde **LLM Model** dropdown'unu gÃ¶rÃ¼n
3. Ä°stediÄŸiniz modeli seÃ§in
4. Model anÄ±nda deÄŸiÅŸir (yeniden baÅŸlatma gerekmez!)

### 3. Programatik Model DeÄŸiÅŸtirme

API ile model deÄŸiÅŸtirebilirsiniz:

```bash
curl -X POST http://localhost:5001/api/model/select \
  -H "Content-Type: application/json" \
  -d '{"model": "deepseek-r1:14b"}'
```

## Test

### Ollama BaÄŸlantÄ± Testi

```bash
./venv/bin/python test_ollama.py
```

Bu test:
- âœ… Ollama baÄŸlantÄ±sÄ±nÄ± kontrol eder
- âœ… Mevcut modelleri listeler
- âœ… Kategorileme doÄŸruluÄŸunu test eder
- âœ… Relevance scoring'i test eder
- âœ… Model deÄŸiÅŸtirmeyi test eder

### Beklenen Ã‡Ä±ktÄ±

```
Testing Ollama Integration
===========================================================

1. Checking Ollama connection...
   Ollama Available: True

2. Listing Available Models...
   Found 10 models:
   - gemma3:12b
   - deepseek-r1:14b
   - qwen2.5:7b

3. Testing Categorization...
   Trend: 'Tesla earnings report Q4 2024'
   Category: Business âœ“

4. Testing Relevance Scoring...
   Item: 'Major earthquake hits California'
   Relevance: 0.95 âœ“
```

## Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

GerÃ§ek test sonuÃ§larÄ±:

### Kategorileme DoÄŸruluÄŸu

| Model | DoÄŸruluk | HÄ±z |
|-------|----------|-----|
| qwen2.5-coder:1.5b | ~85% | 0.5s |
| phi4-mini:3.8b | ~90% | 0.8s |
| qwen2.5:7b | ~95% | 1.5s |
| gemma3:12b | ~98% | 2.5s |
| deepseek-r1:14b | ~99% | 3.5s |

### Relevance Scoring DoÄŸruluÄŸu

| Model | Hassasiyet | HÄ±z |
|-------|------------|-----|
| qwen2.5-coder:1.5b | Orta | 0.3s |
| phi4-mini:3.8b | Ä°yi | 0.5s |
| qwen2.5:7b | Ã‡ok Ä°yi | 1.0s |
| gemma3:12b | MÃ¼kemmel | 2.0s |
| deepseek-r1:14b | MÃ¼kemmel | 3.0s |

## Ã–neriler

### GÃ¼nlÃ¼k KullanÄ±m

**HÄ±zlÄ± ve verimli:**
```bash
ollama pull qwen2.5:7b
```
- Dengeli performans
- Orta RAM kullanÄ±mÄ±
- Ä°yi doÄŸruluk

### YÃ¼ksek DoÄŸruluk

**Maksimum kalite:**
```bash
ollama pull deepseek-r1:14b
```
- En yÃ¼ksek doÄŸruluk
- YavaÅŸ ama emin
- YÃ¼ksek RAM gerekir

### DÃ¼ÅŸÃ¼k Kaynak

**Minimum sistem:**
```bash
ollama pull phi4-mini:3.8b-fp16
```
- HÄ±zlÄ±
- Az RAM
- Kabul edilebilir kalite

## Sorun Giderme

### Ollama Ã‡alÄ±ÅŸmÄ±yor

```bash
# Ollama'yÄ± kontrol et
ollama --version

# Servisi yeniden baÅŸlat
pkill ollama
ollama serve
```

### Model Ä°ndirilemedi

```bash
# Disk alanÄ±nÄ± kontrol et
df -h

# Model listesini kontrol et
ollama list

# Modeli yeniden indir
ollama pull model-adi
```

### Model BulunamadÄ± (404)

EÄŸer test sÄ±rasÄ±nda 404 hatasÄ± alÄ±yorsanÄ±z:

1. Model gerÃ§ekten indirilmiÅŸ mi kontrol edin:
```bash
ollama list
```

2. Model adÄ±nÄ± tam yazÄ±n:
```bash
# âŒ YanlÄ±ÅŸ
ollama pull gemma3

# âœ… DoÄŸru
ollama pull gemma3:12b
```

### YavaÅŸ Performans

1. **Daha kÃ¼Ã§Ã¼k model kullanÄ±n:**
```bash
ollama pull qwen2.5-coder:1.5b
```

2. **GPU kullanÄ±mÄ±nÄ± kontrol edin:**
```bash
# macOS Metal
export OLLAMA_METAL=1

# NVIDIA GPU
export OLLAMA_CUDA=1
```

3. **Concurrency'yi azaltÄ±n** - `ollama_analyzer.py` iÃ§inde:
```python
max_concurrent = 1  # VarsayÄ±lan 3, azaltÄ±n
```

## API Endpoints

Uygulama Ã§alÄ±ÅŸÄ±rken bu endpoint'ler kullanÄ±labilir:

### Model Listesi
```bash
curl http://localhost:5001/api/models
```

DÃ¶nen yanÄ±t:
```json
{
  "models": [
    {"name": "gemma3:12b"},
    {"name": "deepseek-r1:14b"}
  ],
  "current_model": "gemma3:12b"
}
```

### Model DeÄŸiÅŸtir
```bash
curl -X POST http://localhost:5001/api/model/select \
  -H "Content-Type: application/json" \
  -d '{"model": "deepseek-r1:14b"}'
```

DÃ¶nen yanÄ±t:
```json
{
  "success": true,
  "model": "deepseek-r1:14b"
}
```

## Avantajlar vs Gemini

| Ã–zellik | Ollama | Gemini |
|---------|--------|--------|
| **Maliyet** | âœ… Ãœcretsiz | âŒ Ãœcretli (quota aÅŸÄ±mÄ±) |
| **HÄ±z** | âœ… Ã‡ok hÄ±zlÄ± (yerel) | âš ï¸ Network latency |
| **Gizlilik** | âœ… Tamamen yerel | âŒ Veriler Google'a gider |
| **Rate Limit** | âœ… Yok | âŒ 10 req/min (free) |
| **Internet** | âœ… Gerekmez | âŒ Gerekir |
| **DoÄŸruluk** | â­â­â­â­ | â­â­â­â­â­ |
| **Kurulum** | âš ï¸ Model indirmek gerekir | âœ… Hemen kullanÄ±m |

## SonuÃ§

Ollama ile:
- âœ… API key yok
- âœ… Rate limit yok
- âœ… Ãœcretsiz sÄ±nÄ±rsÄ±z kullanÄ±m
- âœ… HÄ±zlÄ± ve yerel
- âœ… Web arayÃ¼zÃ¼nden model deÄŸiÅŸtirme

**Hemen baÅŸlayÄ±n:**
```bash
# 1. Ollama'yÄ± baÅŸlat
ollama serve

# 2. Model indir
ollama pull qwen2.5:7b

# 3. UygulamayÄ± Ã§alÄ±ÅŸtÄ±r
./start.sh

# 4. Web arayÃ¼zÃ¼nÃ¼ aÃ§
open http://localhost:5001
```
